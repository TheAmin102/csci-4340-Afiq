{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is try to do IQR techniquec. and rerun the result back again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after IQR outlier removal saved to 'feature_selection2_after_IQR.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2.csv')\n",
    "\n",
    "# Separate the target variable\n",
    "target_variable = 'PRICE'\n",
    "y = df[target_variable]\n",
    "X = df.drop(target_variable, axis=1)\n",
    "\n",
    "# Apply IQR outlier detection\n",
    "Q1 = X.quantile(0.25)\n",
    "Q3 = X.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))\n",
    "X_filtered = X[~outliers]\n",
    "\n",
    "# Combine filtered features and target variable\n",
    "df_filtered = pd.concat([X_filtered, y], axis=1)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "df_filtered.to_csv(\"feature_selection2_after_IQR.csv\", index=False)\n",
    "\n",
    "print(\"Dataframe after IQR outlier removal saved to 'feature_selection2_after_IQR.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       LOCATION  REGION    SQFT  WGTADJ   PRICE\n",
       "0            1       3  1200.0     NaN   90000\n",
       "1            2       3  1200.0     NaN   82000\n",
       "2            3       3  1100.0     NaN   68000\n",
       "3            2       4  1200.0     NaN   90000\n",
       "4            1       4  1200.0     NaN   90000\n",
       "...        ...     ...     ...     ...     ...\n",
       "2753         2       4  1400.0  1.0439  140000\n",
       "2754         1       1  1600.0  1.0439  150000\n",
       "2755         1       1  1600.0  1.0439  150000\n",
       "2756         1       1  1500.0  1.0439  220000\n",
       "2757         3       2  1100.0  1.0439  110000\n",
       "\n",
       "[2758 rows x 5 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2_after_IQR.csv')\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after Z-score outlier removal saved to 'feature_selection2_after_Zscore.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2.csv')\n",
    "\n",
    "# Separate the target variable\n",
    "target_variable = 'PRICE'\n",
    "y = df[target_variable]\n",
    "X = df.drop(target_variable, axis=1)\n",
    "\n",
    "# Apply Z-score outlier detection\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "z_scores = (X - mean) / std\n",
    "outliers = (abs(z_scores) > 3)  # Threshold for outliers: 3 standard deviations\n",
    "X_filtered = X[~outliers.any(axis=1)]  # Filter out rows with any outlier value\n",
    "\n",
    "# Combine filtered features and target variable\n",
    "df_filtered = pd.concat([X_filtered, y], axis=1)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "df_filtered.to_csv(\"feature_selection2_after_Zscore.csv\", index=False)\n",
    "\n",
    "print(\"Dataframe after Z-score outlier removal saved to 'feature_selection2_after_Zscore.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       LOCATION  REGION    SQFT  WGTADJ   PRICE\n",
       "0          1.0     3.0  1200.0  0.9369   90000\n",
       "1          2.0     3.0  1200.0  0.9369   82000\n",
       "2          3.0     3.0  1100.0  0.9369   68000\n",
       "3          2.0     4.0  1200.0  0.9369   90000\n",
       "4          1.0     4.0  1200.0  0.9369   90000\n",
       "...        ...     ...     ...     ...     ...\n",
       "2753       NaN     NaN     NaN     NaN  230000\n",
       "2754       NaN     NaN     NaN     NaN  260000\n",
       "2755       NaN     NaN     NaN     NaN  280000\n",
       "2756       NaN     NaN     NaN     NaN  350000\n",
       "2757       NaN     NaN     NaN     NaN  240000\n",
       "\n",
       "[2758 rows x 5 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2_after_Zscore.csv')\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do data scale with new data after remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      LOCATION    REGION      SQFT    WGTADJ   PRICE\n",
      "0    -0.969025  0.304856 -0.614382       NaN   90000\n",
      "1     0.158166  0.304856 -0.614382       NaN   82000\n",
      "2     1.285358  0.304856 -0.873189       NaN   68000\n",
      "3     0.158166  1.232883 -0.614382       NaN   90000\n",
      "4    -0.969025  1.232883 -0.614382       NaN   90000\n",
      "...        ...       ...       ...       ...     ...\n",
      "2753  0.158166  1.232883 -0.096769  0.228598  140000\n",
      "2754 -0.969025 -1.551198  0.420844  0.228598  150000\n",
      "2755 -0.969025 -1.551198  0.420844  0.228598  150000\n",
      "2756 -0.969025 -1.551198  0.162038  0.228598  220000\n",
      "2757  1.285358 -0.623171 -0.873189  0.228598  110000\n",
      "\n",
      "[2758 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2_after_IQR.csv')\n",
    "\n",
    "# Separate the target variable\n",
    "target_variable = 'PRICE'\n",
    "y = df[target_variable]\n",
    "X = df.drop(target_variable, axis=1)\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features (excluding the target variable)\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform the features using the scaler\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Convert the scaled features back into a DataFrame\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Optional: Combine the scaled features with the target variable\n",
    "df_scaled = pd.concat([X_scaled, y], axis=1)\n",
    "\n",
    "# Print or save the scaled DataFrame\n",
    "print(df_scaled)  # Or use \n",
    "df_scaled.to_csv('scaled_features3.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check how much rows i have if i decide to use IQR or z-score. because using both of them will made my dataset have NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 10\n",
      "Number of rows after removing NaN values: 2748\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2_after_Zscore.csv')\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"Number of rows with NaN values:\", df.isnull().any(axis=1).sum())\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_without_NaN = df.dropna()\n",
    "\n",
    "# Count remaining rows\n",
    "print(\"Number of rows after removing NaN values:\", df_without_NaN.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 478\n",
      "Number of rows after removing NaN values: 2280\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('feature_selection2_after_IQR.csv')\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"Number of rows with NaN values:\", df.isnull().any(axis=1).sum())\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_without_NaN = df.dropna()\n",
    "\n",
    "# Count remaining rows\n",
    "print(\"Number of rows after removing NaN values:\", df_without_NaN.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('scaled_features3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m lasso_regressor \u001b[38;5;241m=\u001b[39m Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)  \u001b[38;5;66;03m# You may need to tune the alpha parameter\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Fit the Lasso model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mlasso_regressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Predict on the test set using Lasso\u001b[39;00m\n\u001b[0;32m     26\u001b[0m lasso_pred \u001b[38;5;241m=\u001b[39m lasso_regressor\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:905\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[0;32m    904\u001b[0m     X_copied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept\n\u001b[1;32m--> 905\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_copied\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    915\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    916\u001b[0m         y, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    917\u001b[0m     )\n\u001b[0;32m    919\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLasso does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regressor\n",
    "lasso_regressor = Lasso(alpha=0.01)  # You may need to tune the alpha parameter\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using Lasso\n",
    "lasso_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost\n",
    "xgb_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Set the weights for the hybrid model\n",
    "lasso_weight = 0.65\n",
    "xgb_weight = 0.35\n",
    "\n",
    "# Combine predictions with weights\n",
    "hybrid_pred = lasso_weight * lasso_pred + xgb_weight * xgb_pred\n",
    "\n",
    "# Calculate metrics for the hybrid model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "rae = mean_absolute_error(y_test, hybrid_pred)\n",
    "mape = np.mean(np.abs((y_test - hybrid_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regressor\n",
    "lasso_regressor = Lasso(alpha=0.01)  # You may need to tune the alpha parameter\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using Lasso\n",
    "lasso_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost\n",
    "xgb_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Set the weights for the hybrid model\n",
    "lasso_weight = 0.65\n",
    "xgb_weight = 0.35\n",
    "\n",
    "# Combine predictions with weights\n",
    "hybrid_pred = lasso_weight * lasso_pred + xgb_weight * xgb_pred\n",
    "\n",
    "# Calculate metrics for the hybrid model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "rae = mean_absolute_error(y_test, hybrid_pred)\n",
    "mape = np.mean(np.abs((y_test - hybrid_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regressor\n",
    "lasso_regressor = Lasso(alpha=0.01)  # You may need to tune the alpha parameter\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using Lasso\n",
    "lasso_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost\n",
    "xgb_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Set the weights for the hybrid model\n",
    "lasso_weight = 0.65\n",
    "xgb_weight = 0.35\n",
    "\n",
    "# Combine predictions with weights\n",
    "hybrid_pred = lasso_weight * lasso_pred + xgb_weight * xgb_pred\n",
    "\n",
    "# Calculate metrics for the hybrid model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "rae = mean_absolute_error(y_test, hybrid_pred)\n",
    "mape = np.mean(np.abs((y_test - hybrid_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Ridge Regression model\n",
    "ridge_regressor = Ridge(alpha=1.0)  # Set the regularization strength (alpha)\n",
    "\n",
    "# Fit the model\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('scaled_features2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize the LASSO Regression model\n",
    "lasso_regressor = Lasso(alpha=0.1)  # Set the regularization strength (alpha)\n",
    "\n",
    "# Fit the model\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

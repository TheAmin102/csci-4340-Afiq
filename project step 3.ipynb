{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 21806.11\n",
      "Relative Absolute Error (RAE): 10845.12\n",
      "Mean Absolute Percentage Error (MAPE): 8.01%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 0.15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 22596.42\n",
      "Relative Absolute Error (RAE): 12184.99\n",
      "Mean Absolute Percentage Error (MAPE): 4541.26%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_7396\\853481969.py:33: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Machine (LightGBM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 540\n",
      "[LightGBM] [Info] Number of data points in the train set: 5618, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88623.193129\n",
      "Root Mean Squared Error (RMSE): 21389.61\n",
      "Relative Absolute Error (RAE): 11087.42\n",
      "Mean Absolute Percentage Error (MAPE): 2021.96%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_7396\\3591373865.py:33: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the LightGBM Regressor\n",
    "lgbm_regressor = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lgbm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lgbm_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Regression 65% Lasso and 35% XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.144e+12, tolerance: 3.303e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 24260.49\n",
      "Relative Absolute Error (RAE): 13592.08\n",
      "Mean Absolute Percentage Error (MAPE): 16120.31%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 2.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_7396\\2722885552.py:50: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regressor\n",
    "lasso_regressor = Lasso(alpha=0.01)  # You may need to tune the alpha parameter\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using Lasso\n",
    "lasso_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost\n",
    "xgb_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Set the weights for the hybrid model\n",
    "lasso_weight = 0.65\n",
    "xgb_weight = 0.35\n",
    "\n",
    "# Combine predictions with weights\n",
    "hybrid_pred = lasso_weight * lasso_pred + xgb_weight * xgb_pred\n",
    "\n",
    "# Calculate metrics for the hybrid model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "rae = mean_absolute_error(y_test, hybrid_pred)\n",
    "mape = np.mean(np.abs((y_test - hybrid_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 2-level stacking architecture with Random Forest and LightGBM as base models and XGBoost as the meta-model.\n",
    "5-fold cross-validation was used to assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 4494, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 89468.459279\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 539\n",
      "[LightGBM] [Info] Number of data points in the train set: 4494, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88299.647530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 4494, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 89000.985091\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 4495, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88468.425139\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 4495, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 87878.648721\n",
      "Root Mean Squared Error (RMSE): 24270.53\n",
      "Relative Absolute Error (RAE): 12568.44\n",
      "Mean Absolute Percentage Error (MAPE): 692.28%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "rf_base_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "lgbm_base_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create an array to store base model predictions for each fold\n",
    "base_model_predictions = np.zeros((X_train.shape[0], 2))\n",
    "\n",
    "# Initialize meta-model\n",
    "xgb_meta_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 2-level stacking with cross-validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train base models on the current fold\n",
    "    rf_base_model.fit(X_train_fold, y_train_fold)\n",
    "    lgbm_base_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation set for base models\n",
    "    base_model_predictions[val_index, 0] = rf_base_model.predict(X_val_fold)\n",
    "    base_model_predictions[val_index, 1] = lgbm_base_model.predict(X_val_fold)\n",
    "\n",
    "# Train the meta-model on base model predictions\n",
    "xgb_meta_model.fit(base_model_predictions, y_train)\n",
    "\n",
    "# Make predictions on the test set for base models\n",
    "rf_test_pred = rf_base_model.predict(X_test)\n",
    "lgbm_test_pred = lgbm_base_model.predict(X_test)\n",
    "\n",
    "# Combine predictions for the test set as input to the meta-model\n",
    "meta_model_input = np.column_stack((rf_test_pred, lgbm_test_pred))\n",
    "\n",
    "# Make final predictions using the meta-model\n",
    "final_pred = xgb_meta_model.predict(meta_model_input)\n",
    "\n",
    "# Calculate metrics for the stacked model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
    "rae = mean_absolute_error(y_test, final_pred)\n",
    "mape = np.mean(np.abs((y_test - final_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(final_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do all the same but with 0.45 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest with 0.45 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 22894.86\n",
      "Relative Absolute Error (RAE): 11366.24\n",
      "Mean Absolute Percentage Error (MAPE): 8.47%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 0.15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme Gradient Boosting (XGBoost) with 0.45 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 23398.22\n",
      "Relative Absolute Error (RAE): 12430.55\n",
      "Mean Absolute Percentage Error (MAPE): 4670.08%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_11152\\3266553585.py:33: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Machine (LightGBM) with 0.45 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 538\n",
      "[LightGBM] [Info] Number of data points in the train set: 3862, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88784.690316\n",
      "Root Mean Squared Error (RMSE): 22611.99\n",
      "Relative Absolute Error (RAE): 11616.88\n",
      "Mean Absolute Percentage Error (MAPE): 1825.42%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_11152\\2629409274.py:33: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Initialize the LightGBM Regressor\n",
    "lgbm_regressor = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lgbm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lgbm_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Regression 65% Lasso and 35% XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 24363.14\n",
      "Relative Absolute Error (RAE): 13477.09\n",
      "Mean Absolute Percentage Error (MAPE): 13524.11%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 2.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.504e+12, tolerance: 2.288e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_11152\\2405285181.py:50: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regressor\n",
    "lasso_regressor = Lasso(alpha=0.01)  # You may need to tune the alpha parameter\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using Lasso\n",
    "lasso_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost\n",
    "xgb_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Set the weights for the hybrid model\n",
    "lasso_weight = 0.65\n",
    "xgb_weight = 0.35\n",
    "\n",
    "# Combine predictions with weights\n",
    "hybrid_pred = lasso_weight * lasso_pred + xgb_weight * xgb_pred\n",
    "\n",
    "# Calculate metrics for the hybrid model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "rae = mean_absolute_error(y_test, hybrid_pred)\n",
    "mape = np.mean(np.abs((y_test - hybrid_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 2-level stacking architecture with Random Forest and LightGBM as base models and XGBoost as the meta-model.\n",
    "5-fold cross-validation was used to assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 532\n",
      "[LightGBM] [Info] Number of data points in the train set: 3089, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88418.622532\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 535\n",
      "[LightGBM] [Info] Number of data points in the train set: 3089, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88851.101651\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 3090, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88794.466019\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 534\n",
      "[LightGBM] [Info] Number of data points in the train set: 3090, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 88672.150485\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 536\n",
      "[LightGBM] [Info] Number of data points in the train set: 3090, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 89187.013916\n",
      "Root Mean Squared Error (RMSE): 26204.14\n",
      "Relative Absolute Error (RAE): 13674.84\n",
      "Mean Absolute Percentage Error (MAPE): 595.80%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "rf_base_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "lgbm_base_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create an array to store base model predictions for each fold\n",
    "base_model_predictions = np.zeros((X_train.shape[0], 2))\n",
    "\n",
    "# Initialize meta-model\n",
    "xgb_meta_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 2-level stacking with cross-validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train base models on the current fold\n",
    "    rf_base_model.fit(X_train_fold, y_train_fold)\n",
    "    lgbm_base_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation set for base models\n",
    "    base_model_predictions[val_index, 0] = rf_base_model.predict(X_val_fold)\n",
    "    base_model_predictions[val_index, 1] = lgbm_base_model.predict(X_val_fold)\n",
    "\n",
    "# Train the meta-model on base model predictions\n",
    "xgb_meta_model.fit(base_model_predictions, y_train)\n",
    "\n",
    "# Make predictions on the test set for base models\n",
    "rf_test_pred = rf_base_model.predict(X_test)\n",
    "lgbm_test_pred = lgbm_base_model.predict(X_test)\n",
    "\n",
    "# Combine predictions for the test set as input to the meta-model\n",
    "meta_model_input = np.column_stack((rf_test_pred, lgbm_test_pred))\n",
    "\n",
    "# Make final predictions using the meta-model\n",
    "final_pred = xgb_meta_model.predict(meta_model_input)\n",
    "\n",
    "# Calculate metrics for the stacked model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
    "rae = mean_absolute_error(y_test, final_pred)\n",
    "mape = np.mean(np.abs((y_test - final_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(final_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do all the same but with 0.75 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest with 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 23531.41\n",
      "Relative Absolute Error (RAE): 11858.16\n",
      "Mean Absolute Percentage Error (MAPE): 8.74%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 0.16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme Gradient Boosting (XGBoost) with 0.75 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 26027.62\n",
      "Relative Absolute Error (RAE): 14090.04\n",
      "Mean Absolute Percentage Error (MAPE): 4506.89%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_11152\\603248513.py:33: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Machine (LightGBM) with 0.45 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 520\n",
      "[LightGBM] [Info] Number of data points in the train set: 1755, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 87285.944160\n",
      "Root Mean Squared Error (RMSE): 24138.88\n",
      "Relative Absolute Error (RAE): 12597.48\n",
      "Mean Absolute Percentage Error (MAPE): 2241.67%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_11152\\3054711787.py:33: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "\n",
    "# Initialize the LightGBM Regressor\n",
    "lgbm_regressor = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lgbm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lgbm_regressor.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rae = mean_absolute_error(y_test, y_pred)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(y_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Regression 65% Lasso and 35% XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 25429.00\n",
      "Relative Absolute Error (RAE): 14067.49\n",
      "Mean Absolute Percentage Error (MAPE): 12433.48%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.740e+11, tolerance: 1.041e+09\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\theam\\AppData\\Local\\Temp\\ipykernel_11152\\1653759782.py:50: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regressor\n",
    "lasso_regressor = Lasso(alpha=0.01)  # You may need to tune the alpha parameter\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using Lasso\n",
    "lasso_pred = lasso_regressor.predict(X_test)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set using XGBoost\n",
    "xgb_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Set the weights for the hybrid model\n",
    "lasso_weight = 0.65\n",
    "xgb_weight = 0.35\n",
    "\n",
    "# Combine predictions with weights\n",
    "hybrid_pred = lasso_weight * lasso_pred + xgb_weight * xgb_pred\n",
    "\n",
    "# Calculate metrics for the hybrid model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "rae = mean_absolute_error(y_test, hybrid_pred)\n",
    "mape = np.mean(np.abs((y_test - hybrid_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(hybrid_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 2-level stacking architecture with Random Forest and LightGBM as base models and XGBoost as the meta-model.\n",
    "5-fold cross-validation was used to assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 1404, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 87261.180912\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 511\n",
      "[LightGBM] [Info] Number of data points in the train set: 1404, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 86771.851852\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 1404, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 89090.831197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 516\n",
      "[LightGBM] [Info] Number of data points in the train set: 1404, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 87180.702991\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 514\n",
      "[LightGBM] [Info] Number of data points in the train set: 1404, number of used features: 27\n",
      "[LightGBM] [Info] Start training from score 86125.153846\n",
      "Root Mean Squared Error (RMSE): 28724.48\n",
      "Relative Absolute Error (RAE): 15399.14\n",
      "Mean Absolute Percentage Error (MAPE): 1215.77%\n",
      "Root Mean Squared Logarithmic Error (RMSLE): 1.28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('project step 2.csv')\n",
    "\n",
    "# Assuming 'PRICE' is the target variable\n",
    "target_variable = 'PRICE'\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df.drop(target_variable, axis=1)\n",
    "y = df[target_variable]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "rf_base_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "lgbm_base_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create an array to store base model predictions for each fold\n",
    "base_model_predictions = np.zeros((X_train.shape[0], 2))\n",
    "\n",
    "# Initialize meta-model\n",
    "xgb_meta_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 2-level stacking with cross-validation\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train base models on the current fold\n",
    "    rf_base_model.fit(X_train_fold, y_train_fold)\n",
    "    lgbm_base_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation set for base models\n",
    "    base_model_predictions[val_index, 0] = rf_base_model.predict(X_val_fold)\n",
    "    base_model_predictions[val_index, 1] = lgbm_base_model.predict(X_val_fold)\n",
    "\n",
    "# Train the meta-model on base model predictions\n",
    "xgb_meta_model.fit(base_model_predictions, y_train)\n",
    "\n",
    "# Make predictions on the test set for base models\n",
    "rf_test_pred = rf_base_model.predict(X_test)\n",
    "lgbm_test_pred = lgbm_base_model.predict(X_test)\n",
    "\n",
    "# Combine predictions for the test set as input to the meta-model\n",
    "meta_model_input = np.column_stack((rf_test_pred, lgbm_test_pred))\n",
    "\n",
    "# Make final predictions using the meta-model\n",
    "final_pred = xgb_meta_model.predict(meta_model_input)\n",
    "\n",
    "# Calculate metrics for the stacked model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_pred))\n",
    "rae = mean_absolute_error(y_test, final_pred)\n",
    "mape = np.mean(np.abs((y_test - final_pred) / y_test)) * 100\n",
    "rmsle = np.sqrt(np.mean(np.power(np.log1p(y_test) - np.log1p(final_pred), 2)))\n",
    "\n",
    "# Print the results\n",
    "print(\"Root Mean Squared Error (RMSE): {:.2f}\".format(rmse))\n",
    "print(\"Relative Absolute Error (RAE): {:.2f}\".format(rae))\n",
    "print(\"Mean Absolute Percentage Error (MAPE): {:.2f}%\".format(mape))\n",
    "print(\"Root Mean Squared Logarithmic Error (RMSLE): {:.2f}\".format(rmsle))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
